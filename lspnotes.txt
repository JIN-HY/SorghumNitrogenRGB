10:46PM: CNN - Adding the input layer...
10:46PM: CNN - Adding convolutional layer conv1...
10:46PM: CNN - Filter dimensions: [3, 3, 3, 16] Inputs: [16, 479, 273, 3] Outputs: [16, 479, 273, 16]
10:46PM: CNN - Adding pooling layer pool1...
10:46PM: CNN - Outputs: [16, 160, 91, 16]
10:46PM: CNN - Adding batch norm layer bn1...
10:46PM: CNN - Adding convolutional layer conv2...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 32] Inputs: [16, 160, 91, 16] Outputs: [16, 160, 91, 32]
10:46PM: CNN - Adding pooling layer pool2...
10:46PM: CNN - Outputs: [16, 54, 31, 32]
10:46PM: CNN - Adding batch norm layer bn2...
10:46PM: CNN - Adding convolutional layer conv3...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Inputs: [16, 54, 31, 32] Outputs: [16, 54, 31, 32]
10:46PM: CNN - Adding pooling layer pool3...
10:46PM: CNN - Outputs: [16, 18, 11, 32]
10:46PM: CNN - Adding batch norm layer bn3...
10:46PM: CNN - Adding convolutional layer conv4...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Inputs: [16, 18, 11, 32] Outputs: [16, 18, 11, 32]
10:46PM: CNN - Adding pooling layer pool4...
10:46PM: CNN - Outputs: [16, 9, 6, 32]
10:46PM: CNN - Adding batch norm layer bn4...
10:46PM: CNN - Adding fully connected layer fc1...
10:46PM: CNN - Inputs: [16, 9, 6, 32] Outputs: [16, 64]
10:46PM: CNN - Adding output layer...
10:46PM: CNN - Inputs: [16, 64] Outputs: [16, 16]
/common/schnablelab/hongyujin/lsplab/./lsplab2/lstm.py:11: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.
  self.__model = tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=num_units)
10:46PM: CNN - Adding the input layer...
10:46PM: CNN - Adding convolutional layer decoder-conv1...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 16] Inputs: [16, 1, 1, 17] Outputs: [16, 1, 1, 16]
10:46PM: CNN - Adding upsampling layer decoder-upsample1...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 16] Outputs: [16, 2, 2, 16]
10:46PM: CNN - Adding convolutional layer decoder-conv2...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 32] Inputs: [16, 2, 2, 16] Outputs: [16, 2, 2, 32]
10:46PM: CNN - Adding convolutional layer decoder-conv3...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Inputs: [16, 2, 2, 32] Outputs: [16, 2, 2, 32]
10:46PM: CNN - Adding convolutional layer decoder-conv4...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Inputs: [16, 2, 2, 32] Outputs: [16, 2, 2, 32]
10:46PM: CNN - Adding upsampling layer decoder-upsample2...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Outputs: [16, 4, 4, 32]
10:46PM: CNN - Adding convolutional layer decoder-conv5...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Inputs: [16, 4, 4, 32] Outputs: [16, 4, 4, 32]
10:46PM: CNN - Adding convolutional layer decoder-conv6...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Inputs: [16, 4, 4, 32] Outputs: [16, 4, 4, 32]
10:46PM: CNN - Adding convolutional layer decoder-conv7...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Inputs: [16, 4, 4, 32] Outputs: [16, 4, 4, 32]
10:46PM: CNN - Adding upsampling layer decoder-upsample3...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Outputs: [16, 8, 8, 32]
10:46PM: CNN - Adding convolutional layer decoder-conv8...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 64] Inputs: [16, 8, 8, 32] Outputs: [16, 8, 8, 64]
10:46PM: CNN - Adding convolutional layer decoder-conv9...
10:46PM: CNN - Filter dimensions: [3, 3, 64, 64] Inputs: [16, 8, 8, 64] Outputs: [16, 8, 8, 64]
10:46PM: CNN - Adding convolutional layer decoder-conv10...
10:46PM: CNN - Filter dimensions: [3, 3, 64, 64] Inputs: [16, 8, 8, 64] Outputs: [16, 8, 8, 64]
10:46PM: CNN - Adding upsampling layer decoder-upsample4...
10:46PM: CNN - Filter dimensions: [3, 3, 64, 64] Outputs: [16, 16, 16, 64]
10:46PM: CNN - Adding convolutional layer decoder-conv11...
10:46PM: CNN - Filter dimensions: [3, 3, 64, 64] Inputs: [16, 16, 16, 64] Outputs: [16, 16, 16, 64]
10:46PM: CNN - Adding convolutional layer decoder-conv12...
10:46PM: CNN - Filter dimensions: [3, 3, 64, 64] Inputs: [16, 16, 16, 64] Outputs: [16, 16, 16, 64]
10:46PM: CNN - Adding convolutional layer decoder-conv13...
10:46PM: CNN - Filter dimensions: [3, 3, 64, 64] Inputs: [16, 16, 16, 64] Outputs: [16, 16, 16, 64]
10:46PM: CNN - Adding upsampling layer decoder-upsample5...
10:46PM: CNN - Filter dimensions: [3, 3, 64, 64] Outputs: [16, 32, 32, 64]
10:46PM: CNN - Adding convolutional layer decoder-conv14...
10:46PM: CNN - Filter dimensions: [3, 3, 64, 32] Inputs: [16, 32, 32, 64] Outputs: [16, 32, 32, 32]
10:46PM: CNN - Adding convolutional layer decoder-conv15...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 32] Inputs: [16, 32, 32, 32] Outputs: [16, 32, 32, 32]
10:46PM: CNN - Adding convolutional layer decoder-conv16...
10:46PM: CNN - Filter dimensions: [3, 3, 32, 16] Inputs: [16, 32, 32, 32] Outputs: [16, 32, 32, 16]
10:46PM: CNN - Adding upsampling layer decoder-upsample6...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 16] Outputs: [16, 64, 64, 16]
10:46PM: CNN - Adding convolutional layer decoder-conv17...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 16] Inputs: [16, 64, 64, 16] Outputs: [16, 64, 64, 16]
10:46PM: CNN - Adding upsampling layer decoder-upsample7...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 16] Outputs: [16, 128, 128, 16]
10:46PM: CNN - Adding convolutional layer decoder-conv18...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 16] Inputs: [16, 128, 128, 16] Outputs: [16, 128, 128, 16]
10:46PM: CNN - Adding upsampling layer decoder-upsample8...
10:46PM: CNN - Filter dimensions: [3, 3, 16, 16] Outputs: [16, 256, 256, 16]
10:46PM: CNN - Adding convolutional layer decoder-conv19...
10:46PM: CNN - Filter dimensions: [1, 1, 16, 3] Inputs: [16, 256, 256, 16] Outputs: [16, 256, 256, 3]




                            # Determinant loss
                            all_emb = tf.concat(cnn_embeddings, 0)
                            avg = tf.reduce_mean(all_emb, axis=0)
                            emb_centered = all_emb - avg
                            cov = tf.matmul(tf.transpose(emb_centered), emb_centered) / (self.__batch_size * self.__num_timepoints)

                            # Add a small epsilon to the diagonal to make sure it's invertible
                            cov = tf.linalg.set_diag(cov, (tf.linalg.diag_part(cov) + self.__variance_constant))

                            # Determinant of the covariance matrix
                            emb_cost = tf.linalg.det(cov)

                            # Treatment loss
                            treatment_loss = self.__get_treatment_loss(treatment, predicted_treatment)

                            # Regularization costs
                            cnn_reg_loss = self.feature_extractor.get_regularization_loss()
                            lstm_reg_loss = self.lstm.get_regularization_loss()

                            # Decoder takes the output from the latent space encoder and tries to reconstruct the input
                            reconstructions = [self.__decoder_net.forward_pass(emb) for emb in cnn_embeddings]
                            reconstructions_tensor = tf.concat(reconstructions, axis=0)

                            decoder_out = self.__decoder_net.layers[-1].output_size

                            original_images = tf.image.resize(tf.concat(image_data, axis=0), [decoder_out[1], decoder_out[2]])

                            # A measure of how diverse the reconstructions are
                            _, rec_var = tf.nn.moments(reconstructions_tensor, axes=[0, 1])
                            reconstruction_diversity = tf.math.reduce_mean(rec_var)

                            pretrain_total_loss = tf.math.reduce_sum([treatment_loss, cnn_reg_loss, lstm_reg_loss, emb_cost])





                            reconstruction_losses = tf.math.reduce_mean(tf.square(tf.subtract(original_images, reconstructions_tensor)), axis=[1, 2, 3])
                            reconstruction_loss, reconstruction_var = tf.nn.moments(reconstruction_losses, axes=[0])

                            reconstruction_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'decoder')
                            reconstruction_gradients, _ = self.__get_clipped_gradients(reconstruction_loss, None, optimizer=recon_optimizer, vars=reconstruction_vars)

                            all_reconstruction_gradients.append(reconstruction_gradients)